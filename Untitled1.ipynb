{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "/home/lucastong/.pyenv/versions/anaconda3-2019.07/envs/tf2/bin/python\n",
      "tensorflow version: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%.3g\" % x))\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from utils import *\n",
    "from scipy.sparse import csr_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import Model, regularizers\n",
    "from tensorflow.keras.layers import Flatten, Dense, Embedding, RNN, GRU, Bidirectional, Layer, Dropout\n",
    "from tensorflow import keras\n",
    "from tensorflow_probability import distributions\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import functools\n",
    "import copy\n",
    "from itertools import chain\n",
    "from process_data import load_processed_dataset\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f\"tensorflow version: {tf.__version__}\")\n",
    "    \n",
    "# TENSORFLOW 2 IS A PAIN IN THE ASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrumbelSoftmaxActivation(tf.keras.layers.Layer):\n",
    "    def __init__(self, temp):\n",
    "        super(GrumbelSoftmaxActivation, self).__init__()\n",
    "        self.temp = temp\n",
    "        self.gumbel = distributions.Gumbel(0, 1)\n",
    "    \n",
    "    def call(self, values):\n",
    "        values = tf.nn.softmax(values, axis=1)\n",
    "        grumbel_sample = self.gumbel.sample(values.shape)\n",
    "        softmax_input = (tf.math.log(values)+grumbel_sample)/self.temp\n",
    "        output = tf.nn.softmax(softmax_input, axis=1)\n",
    "        return output\n",
    "\n",
    "class LocalAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_session_len, embedding_size):\n",
    "        super(LocalAttention, self).__init__()\n",
    "\n",
    "        print(\"max_session_len\", max_session_len, \"embedding_size\", embedding_size)\n",
    "        self.max_session_len = max_session_len\n",
    "        self.embedding_size = embedding_size\n",
    "    \n",
    "        self.tanh_layer = Dense(\n",
    "            self.embedding_size,\n",
    "            activation='tanh',\n",
    "            kernel_initializer='glorot_uniform',\n",
    "            name=\"tanh_layer\")\n",
    "        \n",
    "        u_shape = [self.embedding_size]\n",
    "        self.u = self.add_weight(\"importance\", shape=u_shape)\n",
    "\n",
    "    def call(self, values, mask):\n",
    "        batch_size, cur_session_len, item_shape = values.shape[0],values.shape[1],values.shape[2:]\n",
    "        item_dims = len(item_shape)\n",
    "        \n",
    "        tanh_layer = self.tanh_layer(values)\n",
    "        \n",
    "        similarity_vector = tf.tensordot(tanh_layer, self.u, axes=([2], [0]))\n",
    "\n",
    "        similarity_vector = similarity_vector+mask\n",
    "\n",
    "        weights = tf.nn.softmax(similarity_vector, axis=1)\n",
    "        \n",
    "        values_transpose_axes = [i+2 for i in range(item_dims)]+[0, 1]\n",
    "        inv_values_transpose_axes = [item_dims, item_dims+1]+[i for i in range(item_dims)]\n",
    "        weighted_inputs = tf.transpose(\n",
    "            tf.transpose(values, perm=values_transpose_axes)*weights,\n",
    "            perm=inv_values_transpose_axes\n",
    "        )\n",
    "        output = tf.math.reduce_sum(weighted_inputs, axis=1)\n",
    "        return output\n",
    "\n",
    "class TestModel(Model):\n",
    "    def __init__():\n",
    "        super(TestModel, self).__init__()\n",
    "\n",
    "        self.user_embedding_mtx = self.add_weight(\n",
    "            initializer=tf.random_uniform_initializer(minval=-1, maxval=1), \n",
    "            shape=[num_users, user_shape],\n",
    "        )\n",
    "        self.item_embedding_mtx = self.add_weight(\n",
    "            initializer=tf.random_uniform_initializer(minval=-1, maxval=1), \n",
    "            shape=[num_item, item_shape],\n",
    "        )\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"embedding_mtx_shape\", embedding_mtx_shape)\n",
    "        \n",
    "        self.rnn = Bidirectional(GRU(gru_size, return_sequences=True), merge_mode=\"concat\")\n",
    "        self.attention1 = LocalAttention(max_session_len, gru_size)\n",
    "        self.dense2 = Dense(\n",
    "            dense2_size, \n",
    "            activation=\"linear\", \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        self.dense2_act=tf.keras.layers.LeakyReLU()\n",
    "\n",
    "\n",
    "        self.clustering = Dense(\n",
    "            softmax_classes,\n",
    "            activation=\"linear\", \n",
    "            kernel_initializer='GlorotNormal'\n",
    "        )\n",
    "        self.clustering_act=GrumbelSoftmaxActivation(temp)\n",
    "        self.clustering_map = Dense(\n",
    "            embedding_size, \n",
    "            activation=\"linear\", \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "        self.clustering_map_act=tf.keras.layers.LeakyReLU()\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.dense3 = Dense(\n",
    "            dense3_size, \n",
    "            activation=\"linear\", \n",
    "            kernel_initializer='he_normal'\n",
    "        )\n",
    "\n",
    "        self.dense3_act=tf.keras.layers.LeakyReLU()\n",
    "        self.logits = Dense(\n",
    "            max_embedding_key,\n",
    "            activation=\"linear\",\n",
    "            kernel_initializer='GlorotNormal'\n",
    "        )\n",
    "\n",
    "    def call(self, x, mask, training=False):\n",
    "        print(\"xshape\", x.shape.as_list())\n",
    "        word_embeddings = tf.nn.embedding_lookup(self.word_embedding_mtx, x)\n",
    "        print(\"word_embeddings out\", word_embeddings.shape.as_list())\n",
    "        \n",
    "        \n",
    "        rnn_output = self.rnn(word_embeddings)\n",
    "        print(\"rnn shape\", rnn_output.shape.as_list())\n",
    "        attention_output = self.attention1(rnn_output, mask)\n",
    "        print(\"attention shape\", attention_output.shape.as_list())\n",
    "\n",
    "        \n",
    "        if training:\n",
    "            dense2 = Dropout(.5)(self.dense2_act(self.dense2(attention_output)))\n",
    "            print(\"dense2 shape\", dense2.shape.as_list())\n",
    "            clusters = self.clustering_act(self.clustering(dense2))\n",
    "            print(\"clusters shape\", clusters.shape.as_list())\n",
    "            clusterout = self.clustering_map_act(self.clustering_map(clusters))\n",
    "            print(\"cluster shape\", clusterout.shape.as_list())\n",
    "            # cluster_att = tf.concat((word_embeddings, clusterout), axis=2)\n",
    "            cluster_att = tf.concat((dense2, clusterout), axis=1)\n",
    "            print(\"cluster_att shape\", cluster_att.shape.as_list())\n",
    "\n",
    "            dense3 = Dropout(.5)(self.dense3_act(self.dense3(cluster_att)))\n",
    "            print(\"dense3 shape\", dense3.shape.as_list())\n",
    "\n",
    "            logits = self.logits(dense3)\n",
    "            print(\"logits shape\", logits.shape.as_list())\n",
    "\n",
    "        else:\n",
    "            dense2 = self.dense2_act(self.dense2(attention_output))\n",
    "            clusters = self.clustering_act(self.clustering(dense2))\n",
    "            clusterout = self.clustering_map_act(self.clustering_map(clusters))\n",
    "            # cluster_att = tf.concat((word_embeddings, clusterout), axis=2)\n",
    "            cluster_att = tf.concat((dense2, clusterout), axis=1)\n",
    "\n",
    "            dense3 = self.dense3_act(self.dense3(cluster_att))\n",
    "\n",
    "            logits = self.logits(dense3)\n",
    "            \n",
    "            noise = tf.random.uniform(logits.shape, maxval=10e-6)\n",
    "            logits = logits + noise # adding the randomness cause topk categorical acc shitty\n",
    "        \n",
    "        softmax = tf.nn.softmax(logits)\n",
    "        return logits, softmax\n",
    "        \n",
    "    def get_loss(self, y_true, logits):\n",
    "        ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)(y_true, logits)\n",
    "\n",
    "        l1_reg = sum([tf.reduce_sum(tf.math.abs(tf.reshape(weight, [-1]))) for weight in self.trainable_variables])        \n",
    "        num_trainable_variables = np.sum([np.prod(var.shape) for var in self.trainable_variables])\n",
    "        l1_reg /= num_trainable_variables\n",
    "        l1_reg *= 10\n",
    "        loss = ce_loss+l1_reg\n",
    "        return loss\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, sessions, mask, labels, scores):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits, preds = self.call(sessions, mask, training=True)\n",
    "            loss = self.get_loss(y_true=labels, logits=logits)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        gradients = [tf.clip_by_value(grad, -1., 1.) for grad in gradients] # clip grads to stop nan problem\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        for score in scores:\n",
    "            score.update_state(labels, tf.cast(preds, tf.float32))\n",
    "\n",
    "        return preds\n",
    "\n",
    "    @tf.function\n",
    "    def test_step(self, sessions, mask, labels, scores):\n",
    "        logits, preds = self.call(sessions, mask, training=False)\n",
    "        for score in scores:\n",
    "            score.update_state(labels, tf.cast(preds, tf.float32))\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"international\"\n",
    "train_phase = \"0\"\n",
    "test_phase = \"4\"\n",
    "\n",
    "batch_size = 2048\n",
    "print(\"batch_size\", batch_size)\n",
    "\n",
    "X_train, y_train = train4\n",
    "X_val, y_val = val4\n",
    "X_test, y_test = test4\n",
    "\n",
    "model = TestModel(\n",
    "    max_embedding_key=max_embedding_key, \n",
    "    max_session_len=max_session_len, \n",
    "    embedding_size=128,\n",
    "    gru_size=128,\n",
    "    dense2_size=512,\n",
    "    dense3_size=256,\n",
    "    softmax_classes=128,\n",
    "    temp=.01\n",
    ")\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=.0003)\n",
    "\n",
    "# train_loss = tf.keras.metrics.SparseCategoricalCrossentropy(name='train_loss')\n",
    "train_accs = [\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5),\n",
    "    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10),\n",
    "    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=20),\n",
    "]\n",
    "val_accs = [\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5),\n",
    "    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10),\n",
    "    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=20),\n",
    "]\n",
    "test_accs = [\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(),\n",
    "    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5),\n",
    "    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10),\n",
    "    tf.keras.metrics.SparseTopKCategoricalAccuracy(k=20),\n",
    "]\n",
    "\n",
    "EPOCHS = 60\n",
    "num_x = len(X_train)\n",
    "\n",
    "train_accs_rec = defaultdict(lambda: [])\n",
    "val_accs_rec = defaultdict(lambda: [])\n",
    "test_accs_rec = defaultdict(lambda: [])\n",
    "train_losses_rec = []\n",
    "val_losses_rec = []\n",
    "\n",
    "for epoch in tqdm(list(range(EPOCHS))):\n",
    "    print(f\"epoch {epoch}/{EPOCHS}\")\n",
    "\n",
    "    for train_acc in train_accs:\n",
    "        train_acc.reset_states()\n",
    "    for val_acc in val_accs:\n",
    "        val_acc.reset_states()\n",
    "    for test_acc in test_accs:\n",
    "        test_acc.reset_states()\n",
    "\n",
    "    for X, y in tqdm(list(batchify(X_train, y_train, shuffle=True, batch_size=batch_size))):\n",
    "        X, mask = mask_length(X, maskon_vals=0, maskoff_vals=-np.inf)\n",
    "        model.train_step(tf.constant(X), tf.constant(mask), tf.constant(y), train_accs)\n",
    "\n",
    "    for X, y in tqdm(list(batchify(X_val, y_val, shuffle=True, batch_size=batch_size))):\n",
    "        X, mask = mask_length(X, maskon_vals=0, maskoff_vals=-np.inf)\n",
    "        model.test_step(tf.constant(X), tf.constant(mask), tf.constant(y), val_accs)\n",
    "\n",
    "    topn_labels = [\"acc\", \"top5 acc\", \"top10 acc\", \"top20 acc\"]\n",
    "    for label, train_acc, val_acc in zip(topn_labels, train_accs, val_accs):\n",
    "        train_accs_rec[label].append(train_acc.result())\n",
    "        val_accs_rec[label].append(val_acc.result())\n",
    "        print(f\"{label} train: {train_acc.result()}\")\n",
    "        print(f\"{label} val: {val_acc.result()}\")\n",
    "\n",
    "    print(f\"Epoch {epoch+1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('tf2': conda)",
   "language": "python",
   "name": "python37464bittf2conda5b374ca37842424e87cbc2e4927142fe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
